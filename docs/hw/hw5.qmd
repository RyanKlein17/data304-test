---
title: "hw5"
author: "Ryan Klein"
format: html
editor: visual
embed-resource: true
---

## Homework 5

Import Packages

```{python}
import altair as alt
import pandas as pd
from altair import datum
```

```{python}
alt.data_transformers.disable_max_rows()
```

### Part 1: Wide-to-long "Your Turn"

Read in the data

```{python}
jobs_url = "https://cdn.jsdelivr.net/npm/vega-datasets@2.8.0/data/jobs.json"
jobs = pd.read_json(jobs_url)
```

#### Plot 1

```{python}
alt.Chart(jobs).transform_pivot(
    'year',
    groupby=['sex', 'job'],
    value='perc'
).mark_circle().encode(
    alt.X(field = "1950", type = "quantitative").scale(constant = 0.0001, type='symlog'),
    alt.Y(field = "2000", type = "quantitative").scale(constant = 0.0001, type='symlog'),
    tooltip=['job:N', 'sex:N']
).properties(width = 400, height = 400
).facet(
  column = "sex"
).configure_mark(
    opacity=0.4
)
```

#### Plot 2

```{python}
base = alt.Chart(jobs).transform_pivot(
    'year',
    groupby=['sex', 'job', 'year', 'perc'],
    value='perc'
).encode(
    alt.X(field = "year", type = "nominal"),
    alt.Y(field = "perc", type = "quantitative").scale(constant = 0.0001, type='symlog'),
    color="job:N"
).properties(width = 600, height = 100
).transform_filter(
    (datum.job == "Farmer")
)

base2 = alt.Chart(jobs).transform_pivot(
    'year',
    groupby=['sex', 'job', 'year', 'perc'],
    value='perc'
).encode(
    alt.X(field = "year", type = "nominal"),
    alt.Y(field = "perc", type = "quantitative").scale(constant = 0.0001, type='symlog'),
    color="job:N"
).properties(width = 600, height = 100
).transform_filter(
    (datum.job == "Professor")
)

(base.mark_line() + base.mark_circle() + base2.mark_line() + base2.mark_circle()).facet(
  row="sex"
)
```

#### Plot 3

```{python}
base = alt.Chart(jobs).transform_pivot(
    'year',
    groupby=['sex', 'job', 'year', 'perc'],
    value='perc'
).encode(
    alt.X(field = "year", type = "nominal"),
    alt.Y(field = "perc", type = "quantitative").scale(constant = 0.0001, type='symlog'),
    color="job:N"
).properties(width = 600, height = 100
).transform_filter(
    (datum.job == "Statistician / Actuary")
)

base2 = alt.Chart(jobs).transform_pivot(
    'year',
    groupby=['sex', 'job', 'year', 'perc'],
    value='perc'
).encode(
    alt.X(field = "year", type = "nominal"),
    alt.Y(field = "perc", type = "quantitative").scale(constant = 0.0001, type='symlog'),
    color="job:N"
).properties(width = 600, height = 100
).transform_filter(
    (datum.job == "Funeral Director")
)

(base.mark_line() + base.mark_circle() + base2.mark_line() + base2.mark_circle()).facet(
  row="sex"
)
```

### Part 2: Maps

#### Plot 1

```{python}
from vega_datasets import data
countries = alt.topo_feature('https://cdn.jsdelivr.net/npm/world-atlas@2/countries-110m.json', feature='countries')
country_map = alt.Chart(countries).mark_geoshape(
    fill='#ffffff',
    stroke='#aaaaaa'
).project('mercator')

country_map.properties(width = 600, height = 400)
```

```{python}
# Import gapminder data
gap_data = pd.read_json("https://cdn.jsdelivr.net/npm/vega-datasets@1.29.0/data/countries.json")

# Selected data from the year 2000.
gd2000 = gap_data.loc[gap_data['year'] == 2000] 

# We changed the names of some of the countries so that they matched the labels within the map.  There were only two countries that had mislabeled data.
gd2000.at[619, 'country'] = "United States of America"
gd2000.at[189, 'country'] = "Dem. Rep. Congo"
gd2000["country"][619]
gd2000["country"][189]
```

```{python}
country_map.transform_lookup(
  lookup='properties.name',
  from_=alt.LookupData(gd2000, 'country', ['life_expect'])
  ).encode(
    fill = "life_expect:Q"
    ).properties(width = 600, height = 400)
```

#### Plot 2

```{python}
airport_data = pd.read_csv("https://cdn.jsdelivr.net/npm/vega-datasets@v1.29.0/data/airports.csv")
airports_per_state = pd.DataFrame(airport_data.value_counts("state"))
airports_per_state.reset_index(inplace=True)
airports_per_state = airports_per_state.rename(columns={0:"total"})
```

```{python}
states_url = 'https://cdn.jsdelivr.net/npm/us-atlas@3/states-10m.json'
states = alt.topo_feature(states_url , feature = 'states')
state_map = alt.Chart(states).mark_geoshape(
    fill = 'transparent',
    stroke = 'steelblue'
).project('albersUsa')

state_map.properties(width = 500, height = 300)
```

```{python}
import json 
from urllib.request import urlopen
state_json = json.load(urlopen(states_url))
print(type(state_json))
```

```{python}
print(state_json['objects']['states']['geometries'][0])
```

```{python}
conversion_data = pd.read_csv("https://raw.githubusercontent.com/jasonong/List-of-US-States/master/states.csv")
airports_per_state = airports_per_state.merge(conversion_data, left_on='state', right_on='Abbreviation', suffixes=('_left', '_right'))
```

```{python}
state_map.transform_lookup(
  lookup='properties.name',
  from_=alt.LookupData(airports_per_state, 'State', ['total'])
  ).encode(
    fill = "total:Q"
    ).properties(width = 600, height = 400, title='Could there be a Link between Land Mass and Number of Airports?')
```

### Part 3: Visualization Analysis

Plot pulled from R/dataisbeautiful.

<https://www.reddit.com/r/dataisbeautiful/comments/11p3555/oc_size_of_bank_failures_since_2000/?utm_source=share&utm_medium=web2x&context=3>

1.  Created shortly after the collapse of Silicon Valley Bank in 2023, this visualization shows various bank failures throughout history and allows viewers to draw their own conclusions. However, the authors are attempting to guide the readers to fathom the size of the 'Washington Mutual' and the 'Silicon Valley' bank failures in comparison to other bank failures. A story I take from this plot is that Silicon Valley Bank failure is one of the worst in US history. However, there has been at least one larger bank failure in history and the economy recovered, giving us hope that the economy would recover from this bank failure.
2.  This plot does a good job identifying the value of interest by highlighting it with light blue. The goal of the plot was to compare the size of the Silicon Valley Bank failure to other failures in history. The visualization does a nice job doing this by using size to map size of the each failure.
3.  With the skills learned in the course, we would be able to recreate much of this visualization. Some of the recreateable aspects would include the marks, the text, the sizes and the colors.
4.  Not sure if it has been covered, but I am confused on how to implement the structure of the plot. The data appears to be organized into a circle with no apparent pattern that is controlled by a variable.
